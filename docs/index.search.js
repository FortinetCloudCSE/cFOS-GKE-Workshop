var relearn_search_index=[{content:`Fortinet cFOS in GKE (Google Cloud) In this workshop you will learn how to deploy Fortinet’s cFOS in GKE
About TEC Workshops TEC Workshops provide the learner with the opportunity to put into practice newly developed skills in an easy to launch environment that can be used for customer engagements. At a minimum a TEC Workshop will include the following:
A use case description
An integrated lab and demo environment
Informational call-outs for key points to discuss or highlight to a customer Questions that could be asked while giving the TEC Workshop as a demo Points of value that relate the business value to the technical feature A reference architecture(s)
Optional components may be included for certain use cases
The TEC Workshop will not be a completely, self-contained learning experience for a single product. A TEC Workshop will cover features and often multiple products where they relate to the use case of interest.
Deployments will be automated for those tasks that are not salient to the learning or demonstration activity in the use case. For example, for a TEC Workshop focused on Indicators of Compromise, the system may deploy a FortiGate and FortiAnalyzer with configurations for these systems. However, the leaner will have to configure the Event Handlers for IOC setup.
cFOS in GKE (Google Cloud) TEC Workshop Introduction:
cFOS is a containerized version of FortiOS that meets the OCI(Open Container Initiative) standard, allowing it to run under Docker, container(s), and CRI-O runtimes.
cFOS offers Layer 7 security features such as Intrusion Prevention System (IPS), DNS filtering, web filtering and SSL deep inspection. It also provides real-time security updates from FortiGuard, which help detect and prevent cyberattacks, block malicious traffic and provide secure access to resources.
When deployed in Kubernetes (k8s), cFOS can protect
IP traffic from pod egress to the internet. east-west traffic between different pod CIDR subnets. TEC Workshop Objectives Pod egress security is essential for protecting networks and data from potential threats originating from outgoing traffic in Kubernetes clusters.
Here are some reasons why pod egress security is crucial:
Prevent data exfiltration: Without proper egress security controls, a malicious actor could potentially use an application running in a pod to exfiltrate sensitive data from the cluster. Control outgoing traffic: By restricting egress traffic from pods to specific IP addresses or domains, organizations can prevent unauthorized communication with external entities and control access to external resources. Comply with regulatory requirements: Many regulations require organizations to implement controls around outgoing traffic to ensure compliance with data privacy and security regulations. Implementing pod egress security controls can help organizations meet these requirements. Prevent malware infections: A pod compromised by malware could use egress traffic to communicate with external command and control servers, leading to further infections and data exfiltration. Egress security controls can help prevent these types of attacks. In summary, implementing pod egress security controls is a vital part of securing Kubernetes clusters and ensuring the integrity, confidentiality, and availability of organizational data. In this use case, applications can route traffic through a dedicated network created by Multus to the cFOS pod. The cFOS pod inspects packets for IPS attacks, URL filtering, DNS filtering, and performs deep packet inspection for SSL encrypted traffic.
Warning The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various Fortinet and Google Cloud services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments without full understanding of how they operate. `,description:"",tags:null,title:"Fortinet cFOS in GKE (Google Cloud)",uri:"/index.html"},{content:'Create and apply license for cFOS Create cFOS license with FortiGate VM license and generate configmap for cFOS to fetch license. One can upload FortiGate VM license to gcloud shell via gcloud SHELL Terminal “upload” feature.\nUpload your FortiGate VM license to the directory where you run your script.\nexport cfos_license_input_file=“path_to_your_license” to setup the environment variable for license. export cfos_license_input_file="path_to_your_license" After exporting the variable, run below command which will apply the license Below command will create application deployment\n[[ -z $cfos_license_input_file ]] \u0026\u0026 cfos_license_input_file="FGVMULTM23000044.lic" [[ -f $cfos_license_input_file ]] || echo $cfos_license_input_file does not exist mkdir -p $HOME/license file="$HOME/license/cfos_license.yaml" licensestring=$(sed \'1d;$d\' $cfos_license_input_file | tr -d \'\\n\') cat \u003c\u003cEOF \u003e$file apiVersion: v1 kind: ConfigMap metadata: name: fos-license labels: app: fos category: license data: license: | -----BEGIN FGT VM LICENSE----- $licensestring -----END FGT VM LICENSE----- EOF #file="$HOME/license/dockerpullsecret.yaml" #[ -e $file ] \u0026\u0026 kubectl create -f $file || echo "$file does not exist" file="$HOME/license/cfos_license.yaml" [ -e $file ] \u0026\u0026 kubectl create -f $file || echo "$file does not exist" output will be similar as below\nValidate License kubectl get cm fos-license output will be similar as below\n',description:"",tags:null,title:"Task 1 - Apply license for cFOS",uri:"/05chapter5/2_task1.html"},{content:`Create Demo Application deployment We use annotation k8s.v1.cni.cncf.io/networks: ‘[ { “name”: “cfosapp” } ]’ to config to POD for secondary interface and custom route entry.
We did not touch pod default route, instead we only insert custom route that we are interested. so for destination, the next hop will be cFOS. cFOS will inspect traffic for those traffic.
When POD attach to cfosapp, it will obtain { “dst”: “104.18.8.132/32”, “gw”: “10.1.200.252”},, { “dst”: “104.18.9.132/32”, “gw”: “10.1.200.252”},, { “dst”: “1.1.1.1/32”, “gw”: “10.1.200.252”} route point to cFOS for inspection in this demo.
Below command will create application deployment
cat \u003c\u003c EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: multitool01-deployment labels: app: multitool01 spec: replicas: 4 selector: matchLabels: app: multitool01 template: metadata: labels: app: multitool01 annotations: k8s.v1.cni.cncf.io/networks: '[ { "name": "cfosapp" } ]' spec: containers: - name: multitool01 image: praqma/network-multitool #image: praqma/network-multitool imagePullPolicy: Always #command: ["/bin/sh","-c"] args: - /bin/sh - -c - /usr/sbin/nginx -g "daemon off;" securityContext: privileged: true EOF kubectl rollout status deployment multitool01-deployment output will be similar as below
`,description:"",tags:null,title:"Task 1 - Create Demo Application",uri:"/04chapter4/2_task1.html"},{content:`Create \u0026 Validate VPC Network for GKE Cluster Create VPC Network for GKE VM instances.
ipcidrRange is the ip range for VM node.
firewallallowProtocol=all will allow ssh into Worker node from anywhere to all protocols.
Below command will Create VPC Networks, Subnets and Firewall-rules
gcloud compute networks create gkenetwork --subnet-mode custom --bgp-routing-mode regional gcloud compute networks subnets create gkenode --network=gkenetwork --range=10.0.0.0/24 gcloud compute firewall-rules create gkenetwork-allow-custom --network gkenetwork --allow all --direction ingress --priority 100 output will be similar as below
Validate VPC Network
gcloud compute networks list --format json --filter gkenetwork output will be similar as below
Validate Subnets
gcloud compute networks subnets list --format json --filter gkenode output will be similar as below
Validate Firewall Rules
gcloud compute firewall-rules list --format json --filter gke output will be similar as below
`,description:"",tags:null,title:"Task 1 - Create VPC Network",uri:"/02chapter2/2_task1.html"},{content:`Install Multus CNI We need to install Multus CNI to route traffic from application POD to cFOS POD.
By default, GKE come with default CNI which uses ptp binary with host-local ipam.
The default CNI config has name 10-containerd-net.conflist When we install Multus, the default Multus config will use –multus-conf-file=auto With the above option, Multus will automatically create 00-multus.conf file with delegate to default 10-containerd-net.conflist We need to change default Multus config path: /home/kubernetes/bin. This is because GKE will only grant this directory with write permission.
Each Worker node will have one Multus POD installed.
Info For the demo application, we will use default behavior. Below command will install Multus CNI
file="multus_auto.yml" multusconfig="auto" multus_bin_hostpath="/home/kubernetes/bin" cat \u003c\u003c EOF \u003e $file # Note: # This deployment file is designed for 'quickstart' of multus, easy installation to test it, # hence this deployment yaml does not care about following things intentionally. # - various configuration options # - minor deployment scenario # - upgrade/update/uninstall scenario # Multus team understand users deployment scenarios are diverse, hence we do not cover # comprehensive deployment scenario. We expect that it is covered by each platform deployment. --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: network-attachment-definitions.k8s.cni.cncf.io spec: group: k8s.cni.cncf.io scope: Namespaced names: plural: network-attachment-definitions singular: network-attachment-definition kind: NetworkAttachmentDefinition shortNames: - net-attach-def versions: - name: v1 served: true storage: true schema: openAPIV3Schema: description: 'NetworkAttachmentDefinition is a CRD schema specified by the Network Plumbing Working Group to express the intent for attaching pods to one or more logical or physical networks. More information available at: https://github.com/k8snetworkplumbingwg/multi-net-spec' type: object properties: apiVersion: description: 'APIVersion defines the versioned schema of this represen tation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: 'NetworkAttachmentDefinition spec defines the desired state of a network attachment' type: object properties: config: description: 'NetworkAttachmentDefinition config is a JSON-formatted CNI configuration' type: string --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: multus rules: - apiGroups: ["k8s.cni.cncf.io"] resources: - '*' verbs: - '*' - apiGroups: - "" resources: - pods - pods/status verbs: - get - update - apiGroups: - "" - events.k8s.io resources: - events verbs: - create - patch - update --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: multus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: multus subjects: - kind: ServiceAccount name: multus namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: multus namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: multus-cni-config namespace: kube-system labels: tier: node app: multus data: # NOTE: If you'd prefer to manually apply a configuration file, you may create one here. # In the case you'd like to customize the Multus installation, you should change the arguments to the Multus pod # change the "args" line below from # - "--multus-conf-file=auto" # to: # "--multus-conf-file=/tmp/multus-conf/07-multus.conf" # Additionally -- you should ensure that the name "07-multus.conf" is the alphabetically first name in the # /etc/cni/net.d/ directory on each node, otherwise, it will not be used by the Kubelet. cni-conf.json: | { "name": "multus-cni-network", "type": "multus", "capabilities": { "portMappings": true }, "delegates": [ { "cniVersion": "0.3.1", "name": "k8s-pod-network", "plugins": [ { "type": "ptp", "mtu": 1460, "ipam": { "type": "host-local", "subnet": "10.140.0.0/24", "gateway": "10.140.0.1", "routes": [ { "dst": "10.144.0.0/20" }, { "dst": "10.140.0.0/14" }, { "dst": "0.0.0.0/0" } ] } }, { "type": "portmap", "capabilities": { "portMappings": true } } ] } ], "kubeconfig": "/etc/cni/net.d/multus.d/multus.kubeconfig" } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-multus-ds namespace: kube-system labels: tier: node app: multus name: multus spec: selector: matchLabels: name: multus updateStrategy: type: RollingUpdate template: metadata: labels: tier: node app: multus name: multus spec: hostNetwork: true tolerations: - operator: Exists effect: NoSchedule - operator: Exists effect: NoExecute serviceAccountName: multus containers: - name: kube-multus image: ghcr.io/k8snetworkplumbingwg/multus-cni:v3.9.3 command: ["/entrypoint.sh"] args: - "--multus-conf-file=$multusconfig" #- "--multus-conf-file=auto" - "--cni-version=0.3.1" resources: requests: cpu: "100m" memory: "50Mi" limits: cpu: "100m" memory: "50Mi" securityContext: privileged: true volumeMounts: - name: cni mountPath: /host/etc/cni/net.d - name: cnibin mountPath: /host/opt/cni/bin - name: multus-cfg mountPath: /tmp/multus-conf initContainers: - name: install-multus-binary image: ghcr.io/k8snetworkplumbingwg/multus-cni:v3.9.3 command: - "cp" - "/usr/src/multus-cni/bin/multus" - "/host/opt/cni/bin/multus" resources: requests: cpu: "10m" memory: "15Mi" securityContext: privileged: true volumeMounts: - name: cnibin mountPath: /host/opt/cni/bin mountPropagation: Bidirectional terminationGracePeriodSeconds: 10 volumes: - name: cni hostPath: path: /etc/cni/net.d - name: cnibin hostPath: path: $multus_bin_hostpath - name: multus-cfg configMap: name: multus-cni-config items: - key: cni-conf.json path: 07-multus.conf EOF kubectl create -f $file kubectl rollout status ds/kube-multus-ds -n kube-system output will be similar as below
`,description:"",tags:null,title:"Task 1 - Install Multus CNI",uri:"/03chapter3/2_task1.html"},{content:`Perform \u0026 Validate initial IPS test on a target website It is very common that a malicious POD can geneate malicious traffic targeting external network or VM or physical machine.
While the traffic is often encrypted, when traffic reaches cFOS, cFOS can decrpyt the traffic and look into the IPS signature.
If the signature is matched, cFOS can either block it or pass it with alert depending on the policy configured.
Note In here, we will generate malicious traffic from application POD targeting a testing website.
cFOS will block the traffic and log it.
One can expect to see IPS traffic logged with matched firewall policy id to indicate which policy is in action. Below command will send malicous traffic from application pod
echo -e 'generate traffic to www.hackthebox.eu' kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- dig www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- ping -c 2 www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl --max-time 5 -k -H "User-Agent: () { :; }; /bin/ls" https://www.hackthebox.eu ; done kubectl get pod | grep fos | awk '{print }' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/ips.0 | grep 300 ; done output will be similar as below
Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/ips.0 | grep 300 ; done output will be similar as below
`,description:"",tags:null,title:"Task 1 - Perform initial IPS test",uri:"/06chapter6/2_task1.html"},{content:"Validate Project \u0026 APIs ",description:"",tags:null,title:"Task 1 - Validate Project \u0026 APIs",uri:"/01chapter1/2_validateproject.html"},{content:`Create GKE Cluster Below configuration attributes need to be set
enable-ip-alias will enable to use alias ip on VM for POD IP Address
service-ipv4-cidr is the CIDR for clusterVIP Address
cluster-ipv4-cidr is for POD IP Address scope
kubectl get node -o wide shall show whether the node is in ready state
Below command will Create GKE Cluster
projectName=$(gcloud config list --format="value(core.project)") region=$(gcloud config get compute/region) gcloud services enable container.googleapis.com \u0026\u0026 gcloud container clusters create my-first-cluster-1 \\ --no-enable-basic-auth \\ --cluster-version 1.26.5-gke.1400 \\ --release-channel "stable" \\ --machine-type e2-standard-2 \\ --image-type "UBUNTU_CONTAINERD" \\ --disk-type "pd-balanced" \\ --disk-size "32" \\ --metadata disable-legacy-endpoints=true \\ --scopes "https://www.googleapis.com/auth/devstorage.read_only","https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/servicecontrol","https://www.googleapis.com/auth/service.management.readonly","https://www.googleapis.com/auth/trace.append" \\ --max-pods-per-node "110" \\ --num-nodes 2 \\ --enable-ip-alias \\ --network "projects/$projectName/global/networks/gkenetwork" \\ --subnetwork "projects/$projectName/regions/$region/subnetworks/gkenode" \\ --no-enable-intra-node-visibility \\ --default-max-pods-per-node "110" \\ --no-enable-master-authorized-networks \\ --addons HorizontalPodAutoscaling,HttpLoadBalancing,GcePersistentDiskCsiDriver \\ --enable-autoupgrade \\ --enable-autorepair \\ --max-surge-upgrade 1 \\ --max-unavailable-upgrade 0 \\ --enable-shielded-nodes \\ --services-ipv4-cidr 10.144.0.0/20 \\ --cluster-ipv4-cidr 10.140.0.0/14 output will be similar as below after completion
Screenshot as in GUI
`,description:"",tags:null,title:"Task 2 - Create GKE Cluster",uri:"/02chapter2/3_task2.html"},{content:`Create \u0026 Validate Role and Service Account for cFOS cFOS will require to read configmap permission to get license and also cFOS will require read-secrets permission to get secret to pull cFOS image. Below command will create cFOS Role and Service Account
file="cfos_account.yml" cat \u003c\u003c EOF \u003e $file --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: namespace: default name: configmap-reader rules: - apiGroups: [""] resources: ["configmaps"] verbs: ["get", "watch", "list"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-configmaps namespace: default subjects: - kind: ServiceAccount name: default apiGroup: "" roleRef: kind: ClusterRole name: configmap-reader apiGroup: "" --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: namespace: default name: secrets-reader rules: - apiGroups: [""] # "" indicates the core API group resources: ["secrets"] verbs: ["get", "watch", "list"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-secrets namespace: default subjects: - kind: ServiceAccount name: default apiGroup: "" roleRef: kind: ClusterRole name: secrets-reader apiGroup: "" EOF kubectl create -f $file1. output will be similar as below
Validate output kubectl get rolebinding read-configmaps \u0026\u0026 kubectl get rolebinding read-secrets -o yaml output will be similar as below
`,description:"",tags:null,title:"Task 2 - Create Role and Service Account for cFOS",uri:"/05chapter5/3_task2.html"},{content:`Perform initial Web Filter test on a target website It is very common that a malicious POD can geneate malicious traffic targeting external network or VM or physical machine.
While the traffic is often encrypted, when traffic reaches cFOS, cFOS can decrpyt the traffic and look into the IPS signature.
If the signature is matched, cFOS can either block it or pass it with alert depending on the policy configured.
Note In here, we will generate malicious traffic from application POD targeting a testing website.
cFOS will block the traffic and log it.
One can expect to see IPS traffic logged with matched firewall policy id to indicate which policy is in action. Below command will generate traffic to target website
echo -e 'generate traffic to https://www.casino.org' kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl -k -I https://www.casino.org ; done output will be similar as below
Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/webf.0 | grep 300 ; done output will be similar as below
`,description:"",tags:null,title:"Task 2 - Perform initial Web Filter test",uri:"/06chapter6/3_task2.html"},{content:`Setup Google Cloud Shell Activate Cloud Shell
Click on the icon as shown below screenshot to activate the Cloud Shell, which should connect to the project.
Authorize Cloud Shell
Copy \u0026 Paste the below command to set up the environmental variables.
project=$(gcloud config list --format="value(core.project)") export region="us-central1" export zone="us-central1-a" gcloud config set project $project gcloud config set compute/region $region gcloud config set compute/zone $zone gcloud config list Note Enable APIs if it prompts for. output will be similar as below
Validate Project
gcloud config get project output will be similar as below
Validate Region
gcloud config get compute/region output will be similar as below
Validate Zone
gcloud config get compute/zone output will be similar as below
`,description:"",tags:null,title:"Task 2 - Setup Google Cloud Shell",uri:"/01chapter1/3_task2.html"},{content:"Validate Demo Application deployment Validate Demo Application deployment\nkubectl rollout status deployment multitool01-deployment output will be similar as below\nkubectl get pod -l app=multitool01 output will be similar as below\nnodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=multitool01 --field-selector spec.nodeName=\"$node\" -o jsonpath='{.items[*].metadata.name}') ; kubectl exec -it po/$podName -- ip route \u0026\u0026 kubectl exec -t po/$podName -- ip address ; done output will be similar as below\n",description:"",tags:null,title:"Task 2 - Validate Demo Application",uri:"/04chapter4/3_task2.html"},{content:`Validate Multus CNI installation kubectl rollout status ds/kube-multus-ds -n kube-system kubectl logs ds/kube-multus-ds -c kube-multus -n kube-system output will be similar as below
`,description:"",tags:null,title:"Task 2 - Validate Multus CNI",uri:"/03chapter3/3_task2.html"},{content:`Create \u0026 Validate cFOS DaemonSet Create cFOS as DaemonSet, so each node will have single cFOS POD.
cFOS will be attached to net-attach-def CRD which was created earlier.
cFOS is configured as a ClusterIP service for restapi port.
cFOS will use annotation to attach to net-attach-def CRD cfosdefaultcni5.
k8s.v1.cni.cncf.io/networks means secondary network.
Default interface inside cFOS is net1.
cFOS will have fixed IP 10.1.200.252/32 which is the range of CRD cni configuration.
cFOS can also have a fixed mac address.
Linux capabilities like NET_ADMIN, SYS_AMDIN, NET_RAW are required for ping, sniff and syslog.
cFOS image will be pulled from Docker Hub with pull secret.
Below command will Create cFOS DaemonSet
project=$(gcloud config list --format="value(core.project)") cat \u003c\u003c EOF | kubectl create -f - --- apiVersion: v1 kind: Service metadata: labels: app: fos name: fos-deployment namespace: default spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: fos type: ClusterIP --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fos-deployment labels: app: fos spec: selector: matchLabels: app: fos template: metadata: labels: app: fos annotations: k8s.v1.cni.cncf.io/networks: '[ { "name": "cfosdefaultcni5", "ips": [ "10.1.200.252/32" ], "mac": "CA:FE:C0:FF:00:02" } ]' #k8s.v1.cni.cncf.io/networks: '[ { "name": "cfosdefaultcni5", "ips": [ "10.1.200.252/32" ], "mac": "CA:FE:C0:FF:00:02" } ]' spec: containers: - name: fos image: gcr.io/$project/fos:7231 #image: 732600308177.dkr.ecr.ap-east-1.amazonaws.com/fos:v7231x86 imagePullPolicy: Always securityContext: privileged: true capabilities: add: ["NET_ADMIN","SYS_ADMIN","NET_RAW"] ports: - name: isakmp containerPort: 500 protocol: UDP - name: ipsec-nat-t containerPort: 4500 protocol: UDP volumeMounts: - mountPath: /data name: data-volume imagePullSecrets: volumes: - name: data-volume #persistentVolumeClaim: #claimName: filestore-pvc hostPath: path: /home/kubernetes/cfosdata type: DirectoryOrCreate EOF kubectl rollout status ds/fos-deployment \u0026\u0026 kubectl get pod -l app=fos output will be similar as below
Validate output kubectl rollout status ds/fos-deployment \u0026\u0026 kubectl get pod -l app=fos output will be similar as below
`,description:"",tags:null,title:"Task 3 - Create cFOS DaemonSet",uri:"/05chapter5/4_task3.html"},{content:'Create Docker Image for cFOS in Google Container Registry Enable Container Registry API\ngcloud services enable containerregistry.googleapis.com output will be similar as below\nCopy \u0026 Paste the below command to create Docker Image.\nproject=$(gcloud config list --format="value(core.project)") gsutil cp gs://$project-cfos-bucket/FOS_X64_DOCKER-v7-build0231-FORTINET.tar . docker load \u003c FOS_X64_DOCKER-v7-build0231-FORTINET.tar docker images | grep ^fos PROJECT_ID=$(gcloud config list --format="value(core.project)") docker tag fos:latest gcr.io/$PROJECT_ID/fos:7231 gcloud auth configure-docker docker push gcr.io/$PROJECT_ID/fos:7231 export cfos_image="gcr.io/$PROJECT_ID/fos:7231" echo $cfos_image output will be similar as below\nValidate Docker Images\ndocker images output will be similar as below\n',description:"",tags:null,title:"Task 3 - Create Docker Image for cFOS",uri:"/01chapter1/4_task3.html"},{content:`Create \u0026 Validate net-attach-def for cFOS We will create net-attach-def with mac-vlan CNI, Multus CNI will use this net-attach-def to create network and attach POD to the network. We use host-local as IPAM CNI. The net-attach-def is for cFOS to attach. The CNI config of mac-vlan use bridge mode and associate with ens4 interface on Worker Node. If the Master interface on Worker Node is other than ens4, we need to change that to match the actual one on the Host Node. One can ssh into Worker Node to check Master interface name. Info The net-attach-def has name cfosdefaultcni5. Below command will create net-attach-def
cat \u003c\u003c EOF | kubectl create -f - apiVersion: "k8s.cni.cncf.io/v1" kind: NetworkAttachmentDefinition metadata: name: cfosdefaultcni5 spec: config: '{ "cniVersion": "0.3.1", "type": "macvlan", "master": "ens4", "mode": "bridge", "ipam": { "type": "host-local", "subnet": "10.1.200.0/24", "rangeStart": "10.1.200.251", "rangeEnd": "10.1.200.253", "gateway": "10.1.200.1" } }' EOF kubectl rollout status ds/kube-multus-ds -n kube-system \u0026\u0026 echo "done" kubectl get net-attach-def cfosdefaultcni5 -o yaml output will be similar as below
Validate net-attach-def kubectl get net-attach-def cfosdefaultcni5 -o yaml output will be similar as below
`,description:"",tags:null,title:"Task 3 - Create net-attach-def for cFOS",uri:"/03chapter3/4_task3.html"},{content:"Delete firewall policy using cFOS Rest API One can use cFOS shell to change firewall policy or can also use cFOS restAPI for it.\nAfter deletion of firewall policy, ping 1.1.1.1 from application pod, which will no longer is reachable.\nBelow command will delete firewall policy\nnodeList=$(kubectl get pod -l app=fos -o jsonpath='{.items[*].status.podIP}') kubectl delete cm foscfgfirewallpolicy echo $nodeList apppodname=$(kubectl get pod | grep multi | grep -v termin | awk '{print $1}' | head -1) for i in $nodeList; do { kubectl exec -it po/$apppodname -- curl -X DELETE \"$i/api/v2/cmdb/firewall/policy/300\" } done output will be similar as below\nValidate the result kubectl get pod | grep multi | grep -v termin | awk '{print $1}' | while read line; do echo -e pod $line; kubectl exec -t po/$line -- ping -c1 1.1.1.1 ; done output will be similar as below\n",description:"",tags:null,title:"Task 3 - Delete firewall policy using cFOS API",uri:"/06chapter6/4_task3.html"},{content:`Validate GKE Cluster kubectl get node -o wide output will be similar as below
`,description:"",tags:null,title:"Task 3 - Validate GKE Cluster",uri:"/02chapter2/4_task3.html"},{content:"Check Routing Table and IP Address nodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=fos --field-selector spec.nodeName=\"$node\" -o jsonpath='{.items[*].metadata.name}') ; kubectl exec -it po/$podName -- ip route \u0026\u0026 kubectl exec -t po/$podName -- ip address ; done output will be similar as below\n",description:"",tags:null,title:"Task 4 - Check Routing Table and IP Address",uri:"/05chapter5/5_task4.html"},{content:`Create an POD to update POD source IP to cFOS POD IPs keep changing due to various reasons like scale in/out or when deleted, so use a dedicated POD to keep updating the POD IP address to cFOS address group.
This POD will keep running as a background process, which will update the application POD’s IP that has annoation with net-attach-def as cfosapp to cFOS using cFOS restful API.
API call to cFOS can use either cFOS dns name or cFOS node IPs.
If cFOS uses shared storage for configuration, then recommended to use dns name, otherwise, one needs to update each cFOS POD directly via cFOS POD IP address.
By default, policy_manager uses cFOS POD IP address.
The policy_manager will also create firewallpolicy for target application unless the policy has already createdby gatekeeper.
Note This is only for demo purpose.
The firewall policy created on cFOS has fixed policyID=200
The policy_manager pod use image from 'interbeing/kubectl-cfos:gke_demo_v1'. Below command will create policy_manager
#!/bin/bash filename="18_cfospolicymanager.yml" [[ -z $policymanagerimage ]] \u0026\u0026 policymanagerimage="interbeing/kubectl-cfos:gke_demo_v2" [[ -z $app_nad_annotation ]] \u0026\u0026 app_nad_annotation="cfosapp" [[ -z $cfos_label ]] \u0026\u0026 cfos_label="fos" function wait_for_pod_ready { pod_name=$(kubectl get pods -l app=policy_manager -o jsonpath='{.items[0].metadata.name}') while true; do pod_status=$(kubectl get pods $pod_name -o jsonpath='{.status.phase}') if [[ $pod_status == "Running" ]]; then kubectl get pod -l app=policy_manager break else echo "Waiting for pod to be in Running state..." sleep 5 fi done } cat \u003c\u003c EOF \u003e $filename --- apiVersion: v1 kind: ServiceAccount metadata: name: pod-reader --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: pod-reader rules: - apiGroups: [""] resources: ["pods"] verbs: ["list", "get", "watch"] - apiGroups: [""] resources: ["configmaps"] verbs: ["list","get","watch","create"] - apiGroups: [""] resources: ["nodes"] verbs: ["list", "get", "watch"] - apiGroups: ["apps"] resources: ["daemonsets"] verbs: ["get", "list", "watch", "patch", "update"] - apiGroups: ["constraints.gatekeeper.sh"] resources: ["k8segressnetworkpolicytocfosutmpolicy"] verbs: ["list","get","watch"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: pod-reader roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: pod-reader subjects: - kind: ServiceAccount name: pod-reader namespace: default --- apiVersion: v1 kind: Pod metadata: name: policymanager labels: app: policy_manager spec: serviceAccountName: pod-reader containers: - name: kubectl-container image: $policymanagerimage env: - name: app_label value: $app_nad_annotation EOF kubectl apply -f $filename \u0026\u0026 wait_for_pod_ready \u0026\u0026 kubectl exec -it po/policymanager -- curl -X GET "http://$cfos_label-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy" \u0026\u0026 kubectl exec -it po/policymanager -- curl -X GET "http://$cfos_label-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/addrgrp" Validate the result kubectl get pod policymanager \u0026\u0026 kubectl exec -it po/policymanager -- curl -X GET "http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/addrgrp" output will be similar as below
`,description:"",tags:null,title:"Task 4 - Create a dedicated POD to update",uri:"/06chapter6/5_task4.html"},{content:`Create \u0026 Validate net-attach-def for application We will create net-attach-def with mac-vlan CNI, Multus CNI will use this net-attach-def to create Network and attach POD to the Network. We use host-local as IPAM CNI. The net-attach-def is for application to attach. The CNI config of mac-vlan use bridge mode and associated with ens4 interface on Worker Node. If the master interface on Worker Node is other than ens4, then it needs to be changed. One can ssh into Worker Node to check Master interface name. The net-attach-def has name cfosapp. We also use cfosapp as label in policy manager demo. Info If you change the name to something else, you will also need to change the image for policy manager where *cfosapp* is hard coded in the image script.
In the nad config, we inserted specific custom route *{ "dst": "104.18.8.132/32", "gw": "10.1.200.252"},,{ "dst": "104.18.9.132/32", "gw": "10.1.200.252"},,{ "dst": "1.1.1.1/32", "gw": "10.1.200.252"}*, for traffic destinated to these subnets, the nexthop is cFOS interface ip. Below command will create net-attach-def for application
cat \u003c\u003c EOF | kubectl create -f - apiVersion: "k8s.cni.cncf.io/v1" kind: NetworkAttachmentDefinition metadata: name: cfosapp spec: config: '{ "cniVersion": "0.3.1", "type": "macvlan", "master": "ens4", "mode": "bridge", "ipam": { "type": "host-local", "subnet": "10.1.200.0/24", "routes": [ { "dst": "104.18.0.0/16", "gw": "10.1.200.252" }, { "dst": "89.238.73.97/32", "gw": "10.1.200.252"}, { "dst": "172.67.162.8/32", "gw": "10.1.200.252"}, { "dst": "104.21.42.126/32","gw": "10.1.200.252"}, { "dst": "104.17.0.0/16","gw": "10.1.200.252"}, { "dst": "1.1.1.1/32", "gw": "10.1.200.252"} ], "rangeStart": "10.1.200.20", "rangeEnd": "10.1.200.251", "gateway": "10.1.200.1" } }' EOF kubectl rollout status ds/kube-multus-ds -n kube-system \u0026\u0026 echo "done" kubectl get net-attach-def cfosapp -o yaml output will be similar as below
Validate net-attach-def for application kubectl get net-attach-def cfosapp -o yaml output will be similar as below
`,description:"",tags:null,title:"Task 4 - Create net-attach-def for application",uri:"/03chapter3/5_task4.html"},{content:'Enable ipforwarding on Worker node By default, in GKE , ipforwarding is disabled. Info For **cFOS** to work, we have to enable *ipforwarding* on Worker node.\nFor more detail, check ipv4 forwarding config. To enable ipforwarding, we need to config canIpForward: true for instance profile, for more detail , check https://cloud.google.com/vpc/docs/using-routes#canipforward. Below command will enable ipforwarding\n[[ $defaultClustername == "" ]] \u0026\u0026 defaultClustername="my-first-cluster-1" gkeClusterName=$defaultClustername clustersearchstring=$(gcloud container clusters list --filter=name=$gkeClusterName --format="value(name)" --limit=1) node_list=$(gcloud compute instances list --filter="name~\'$clustersearchstring\'" --format="value(name)" ) projectName=$(gcloud config list --format="value(core.project)") zone=$(gcloud config list --format="value(compute.zone)" --limit=1) for name in $node_list; do { gcloud compute instances export $name \\ --project $projectName \\ --zone $zone \\ --destination=./$name.txt grep -q "canIpForward: true" $name.txt || sed -i \'/networkInterfaces/i canIpForward: true\' $name.txt sed \'/networkInterfaces/i canIpForward: true\' $name.txt gcloud compute instances update-from-file $name \\ --project $projectName \\ --zone $zone \\ --source=$name.txt \\ --most-disruptive-allowed-action=REFRESH echo "done for $name" } done output will be similar as below\n',description:"",tags:null,title:"Task 4 - Enable 'ipforwarding' on Worker node",uri:"/02chapter2/5_task4.html"},{content:`Perform IPS test for 2nd time on a target website We do IPS test again, this time, the policy is created by policymanager which will take into action.
We can check the IPS logs for validation.
The traffic will match with a different policy ID which is 101.
Below command will send malicous traffic from application pod
echo -e "generate traffic to www.hackthebox.eu" kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- dig www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- ping -c 2 www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl --max-time 5 -k -H "User-Agent: () { :; }; /bin/ls" https://www.hackthebox.eu ; done Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/ips.0 | grep policyid=101 ; done output will be similar as below
`,description:"",tags:null,title:"Task 5 - Perform IPS test for 2nd time",uri:"/06chapter6/6_task5.html"},{content:"Validate cFOS license nodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=fos --field-selector spec.nodeName=\"$node\" -o jsonpath='{.items[*].metadata.name}') ; kubectl logs po/$podName ; done output will be similar as below\n",description:"",tags:null,title:"Task 5 - Validate cFOS license",uri:"/05chapter5/6_task5.html"},{content:`Create configmap for cFOS to configure firewall policy cFOS can be configured to use cFOS shell, kubernetes configmap and restApi. here we use configmap to config cFOS there is an issue in this version, the configuration applied via configmap will not take effect until you restart cFOS DS. the firewall policy has policy id set to 300 and source address set to any. once configmap created, cFOS will read the configmap and apply the policy. you can chech the log on cFOS to verify this. delete configmap will not delete the policy on cFOS. you can also edit the policy in configmap use kubectl edit cm foscfgfirewallpolicy to update the policy.
Configmap for cFOS is utilized to configrue firewall policy. Info Currently, there is an issue with configmap while configuring cFOS, due to which, once configuration is applied via configmap, cFOS DS need to be restarted to take the configuration into effect. Firewall policy has policy id set to 300 and source address set to any.
Once configmap is created, cFOS will read the configmap and apply the policy. One can check the logs on cFOS to validate.
To update the policy, one can use configmap by kubectl edit cm foscfgfirewallpolicy.
Info Delete configmap will not delete the policy on cFOS. Below command will create configmap that include firewall policy configuration
cat \u003c\u003c EOF | kubectl create -f - apiVersion: v1 kind: ConfigMap metadata: name: foscfgfirewallpolicy labels: app: fos category: config data: type: partial config: |- config firewall policy edit "300" set utm-status enable set name "pod_to_internet_HTTPS_HTTP" set srcintf any set dstintf eth0 set srcaddr all set dstaddr all set service HTTPS HTTP PING DNS set ssl-ssh-profile "deep-inspection" set ips-sensor "default" set webfilter-profile "default" set av-profile "default" set nat enable set logtraffic all next end EOF kubectl get cm foscfgfirewallpolicy -o yaml output will be similar as below
Validate configmap for cFOS kubectl get configmap foscfgfirewallpolicy -o yaml output will be similar as below
Check cFOS logs for retriving config from configmap nodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=fos --field-selector spec.nodeName="$node" -o jsonpath='{.items[*].metadata.name}') ; kubectl logs po/$podName ; done output will be similar as below
`,description:"",tags:null,title:"Task 6 - Create configmap for cFOS",uri:"/05chapter5/7_task6.html"},{content:`Perform Web Filter test for 2nd time on a target website We do Web Filter test again, this time, the policy is created by policymanager which will take into action.
We can check the Web Filter logs for validation.
The traffic will match with a different policy ID which is 101.
Below command will generate traffic to target website
kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl -k -I https://www.casino.org ; done Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/webf.0 | grep policyid=101 ; done output will be similar as below
`,description:"",tags:null,title:"Task 6 - Perform Web Filter test for 2nd time",uri:"/06chapter6/7_task6.html"},{content:`Modify Worker Node default CNI Config In previous sections, we did not touch application POD’s default route. We were interested on the traffic whose destination is 1.1.1.1 which will send traffic to cFOS, while the rest of the traffic will continue to go to internet via default route.
What about, if we want to send all traffic from application POD to cFOS?
To accomplish it, we will need to insert a default route into application pod, for which, we will use annotation with keyword default-route to the POD definition. But this is not enough, one will also want some other traffic to go to default interface instead of going to cFOS.
For example, the traffic goes to GKE cluster IP and cross POD to POD traffic.
The GKE default CNI has host-local ipam, and inside host-local ipam, one can insert custom route.
In this usecase, we add ClusterIP CIDR range and POD IP CIDR range, and restart multus DaemonSet to update Multus default config.
Below command will modify default GKE CNI config to insert route
set +H clustersearchstring=$(gcloud container clusters list --format="value(name)" --limit=1) namelist=$(gcloud compute instances list --filter="name~$clustersearchstring" --format="value(name)" ) for name in $namelist ; do { route_exists=$(gcloud compute ssh $name --command="sudo grep -E '\\"dst\\": \\"10.144.0.0\\/20\\"|\\"dst\\": \\"10.140.0.0\\/14\\"' /etc/cni/net.d/10-containerd-net.conflist") if [ -z "$route_exists" ]; then gcloud compute ssh $name --command="sudo sed -i '/\\"dst\\": \\"0.0.0.0\\/0\\"/!b;n;N;s/ \\]$/,\\n {\\"dst\\": \\"10.144.0.0\\/20\\"},\\n {\\"dst\\": \\"10.140.0.0\\/14\\"}\\n ]/' /etc/cni/net.d/10-containerd-net.conflist" kubectl rollout restart ds/kube-multus-ds -n kube-system \u0026\u0026 kubectl rollout status ds/kube-multus-ds -n kube-system kubectl logs ds/kube-multus-ds -n kube-system fi } done Validate the result kubectl logs ds/kube-multus-ds -n kube-system output will be similar as below
`,description:"",tags:null,title:"Task 7 - Modify Worker Node default CNI",uri:"/06chapter6/8_task7.html"},{content:`Restart \u0026 Validate cFOS DaemonSet Info Currently, there is an issue with configmap while configuring cFOS, due to which, once configuration is applied via configmap, cFOS DS need to be restarted to take the configuration into effect.
Alternatively, one can shell into cFOS by executing *fcnsh*. Then remove config and add it back as a workaroud. Below command will restart cFOS DaemonSet
kubectl rollout status ds/fos-deployment \u0026\u0026 kubectl rollout restart ds/fos-deployment \u0026\u0026 kubectl rollout status ds/fos-deployment podname=$(kubectl get pod -l app=fos | grep Running | grep fos | cut -d " " -f 1) echo 'check cfos iptables for snat entry' \u0026\u0026 kubectl exec -it po/$podname -- iptables -L -t nat --verbose | grep MASQ echo "check whether application pod can reach " echo "check deployment multi" echo sleep 30 sleep 30 kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do echo pod $line; kubectl exec -t po/$line -- ping -c1 1.1.1.1 ; done echo 'done' output will be similar as below
Validate deployment status of cFOS after the restart kubectl rollout status ds/fos-deployment output will be similar as below
Check Routing Table and IP Address nodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=fos --field-selector spec.nodeName="$node" -o jsonpath='{.items[*].metadata.name}') ; echo $podName ; kubectl exec -it po/$podName -- iptables -L -t nat --verbose | grep MASQ ; done output will be similar as below
`,description:"",tags:null,title:"Task 7 - Restart cFOS DaemonSet",uri:"/05chapter5/8_task7.html"},{content:"Check ping result kubectl get pod | grep multi | grep -v termin | awk '{print $1}' | while read line; do echo pod $line; kubectl exec -t po/$line -- ping -c1 1.1.1.1 ; done output will be similar as below\n",description:"",tags:null,title:"Task 8 - Check ping result",uri:"/05chapter5/9_task8.html"},{content:`Delete current application deployment Below command will delete current application deployment
kubectl get deployment multitool01-deployment \u0026\u0026 kubectl delete deployment multitool01-deployment output will be similar as below
`,description:"",tags:null,title:"Task 8 - Delete application deployment",uri:"/06chapter6/9_task8.html"},{content:`Create application deployment Create application deployment with annotation to use net-attach-def and make config default-route pointing to net-attach-def attached to interface, .i.e. cFOS interface.
The annotation field has context k8s.v1.cni.cncf.io/networks: ‘[ { “name”: “cfosapp”, “default-route”: [“10.1.200.252”] } ]’, with config having default-route nexthop to 10.1.200.252.
Check IP route table on application, to see the default route pointing to cFOS interface.
Below command to create application deployment
cat \u003c\u003c EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: multitool01-deployment labels: app: multitool01 spec: replicas: 4 selector: matchLabels: app: multitool01 template: metadata: labels: app: multitool01 annotations: k8s.v1.cni.cncf.io/networks: '[ { "name": "cfosapp", "default-route": ["10.1.200.252"] } ]' spec: containers: - name: multitool01 image: praqma/network-multitool #image: praqma/network-multitool imagePullPolicy: Always #command: ["/bin/sh","-c"] args: - /bin/sh - -c - /usr/sbin/nginx -g "daemon off;" securityContext: privileged: true EOF kubectl rollout status deployment multitool01-deployment echo "sleep 30 seconds for it will take some time to trigger policymanager to update cfos addressgrp" sleep 30 Validate the result nodeName=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}') \u0026\u0026 for node in $nodeName; do podName=$(kubectl get pods -l app=multitool01 --field-selector spec.nodeName="$node" -o jsonpath='{.items[*].metadata.name}') ; kubectl exec -it po/$podName -- ip route \u0026\u0026 kubectl exec -t po/$podName -- ip address ; done output will be similar as below
`,description:"",tags:null,title:"Task 9 - Create application deployment",uri:"/06chapter6/10_task9.html"},{content:`Fortinet GKE cFOS Workshop Chapter 1 - Provisioning the Google Cloud environment (40min) Provision your Google Cloud Environment, enter your Email address and click Provision
Please enter your email address Provision Warning After submitting, this page will return with a blank email address box and no other indications. Provisioning can take several minutes. \\*\\*\\* __PLEASE DO NOT SUBMIT MULTIPLE TIMES__ \\*\\*\\* When provisioning is complete, one of the following will happen.
You will receive an email with Google Cloud environment credentials. Use those credentials for this environment, even if you have your own. You will receive and email indicating that there are no environments available to utilize. In this case please try again at a later date. You will receive an email indicating that the supplied email address is from an unsupported domain. No email received due to an unexpected error. You can try again or notify the Google Cloud CSE team. Tasks
Validate Project \u0026 APIs Setup Google Cloud Shell Create Docker Image for cFOS `,description:"",tags:null,title:"Chapter 1 - Getting Started",uri:"/01chapter1.html"},{content:"Perform Web Filter test for 3rd time on a target website For this test, we will use a destination that doesn’t match the default route, for example https://xoso.com.vn. cFOS will classify this website as Gambling, and will be blocked by default profile.\nBelow command will provide access to the target website\nkubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl -k -I https://xoso.com.vn ; done Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/webf.0 | grep policyid=101 ; done output will be similar as below\n",description:"",tags:null,title:"Task 10 - Perform Web Filter test for 3rd time",uri:"/06chapter6/11_task10.html"},{content:`cFOS REST API to delete firewall policy We will delete the policy created by policy_manager POD with policy_id 101 using cfos REST API.
After deleting the firewall policy, we will CURL to check whether any firewall policy exist on cFOS POD.
Below command will delete firewall policy
nodeList=$(kubectl get pod -l app=fos -o jsonpath='{.items[*].status.podIP}') for i in $nodeList; do { kubectl exec -it po/policymanager -- curl -X DELETE "$i/api/v2/cmdb/firewall/policy/101" } done Validate the result kubectl exec -it po/policymanager -- curl -X GET http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy/101 output will be similar as below
`,description:"",tags:null,title:"Task 11 - Delete firewall policy",uri:"/06chapter6/12_task11.html"},{content:`Install gatekeeperv3 In this usecase, we will use standard K8s networkpolicy to create firewallpolicy for cFOS.
The networkpolicy submitted by kubectl, will be sent to gatekeeper admission controller, where there is a constraint delpoyed to inspect the policy constraint via constraint template.
If the networkpolicy pass the constrait check, the constraint template will use cFOS Rest API to create the firewall policy.
Below command will install gatekeeper constraint template
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml \u0026\u0026 \\ kubectl rollout status deployment/gatekeeper-audit -n gatekeeper-system \u0026\u0026 \\ kubectl rollout status deployment/gatekeeper-controller-manager -n gatekeeper-system \u0026\u0026 kubectl rollout status deployment/gatekeeper-audit -n gatekeeper-system Validate the result kubectl rollout status deployment/gatekeeper-audit -n gatekeeper-system \u0026\u0026 kubectl rollout status deployment/gatekeeper-controller-manager -n gatekeeper-system \u0026\u0026 kubectl rollout status deployment/gatekeeper-audit -n gatekeeper-system output will be similar as below
`,description:"",tags:null,title:"Task 12 - Install gatekeeperv3",uri:"/06chapter6/13_task12.html"},{content:`Install gatekeeperv3 constraint template This template include a session call targets.
In the targets, it uses repo as policy engine language to parse the policy.
In the repo, we use function http.send to send API request to cFOS.
Note We only need to deploy template once. Below command will delete firewall policy
filename="47_constraint_template.yml" cat \u003c\u003c EOF \u003e $filename apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8segressnetworkpolicytocfosutmpolicy spec: crd: spec: names: kind: K8sEgressNetworkPolicyToCfosUtmPolicy validation: openAPIV3Schema: properties: message: type: string podcidr: type: string cfosegressfirewallpolicy: type: string outgoingport: type: string utmstatus: type: string ipsprofile: type: string avprofile: type: string sslsshprofile: type: string action: type: string srcintf: type: string firewalladdressapiurl: type: string firewallpolicyapiurl: type: string policyid : type: string extraservice: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8segressnetworkpolicytocfosutmpolicy import future.keywords.if import future.keywords.in import future.keywords.contains services := { "HTTP": ["TCP:80"], "HTTPS": ["TCP:443"], "DNS": ["UDP:53"] } get_service(cfosservice) := msg1 { protocol := input.review.object.spec.egress[_].ports[_].protocol port := sprintf("%v",[input.review.object.spec.egress[_].ports[_].port]) key := concat(":", [ protocol, port ]) some service; services[service][_] == key test := { service } cfosservice in test msg1 := cfosservice } myservice[{ "name" : get_service("HTTPS") }] { 1==1 } myservice[{ "name" : get_service("HTTP") }] { 1==1 } myservice[{ "name" : get_service("DNS") }] { 1==1 } myservice[{"name":msg1}] { input.parameters.extraservice=="PING" msg1:="PING" } violation[{ "msg" : msg }] { #the NetworkPolicy must has label under metadata which match the constraint input.review.object.metadata.labels.app==input.parameters.label #GET INPUT from reguar NetworkPolicy for cfos firewall policy namespace := input.review.object.metadata.namespace label := input.review.object.spec.podSelector.matchLabels.app t := concat("",[namespace,"app"]) src_addr_group := concat("",[t,label]) dstipblock := input.review.object.spec.egress[_].to[_].ipBlock.cidr policyname := input.review.object.metadata.name #GET INPUT from constraint template policyid := input.parameters.policyid ipsprofile := input.parameters.ipsprofile avprofile := input.parameters.avprofile sslsshprofile := input.parameters.sslsshprofile action := input.parameters.action srcintf := input.parameters.srcintf utmstatus := input.parameters.utmstatus outgoingport := input.parameters.outgoingport #firewalladdressapiurl := input.parameters.firewalladdressapiurl firewallpolicyapiurl := input.parameters.firewallpolicyapiurl firewalladdrgrpapiurl := input.parameters.firewalladdressgrpapiurl #Begin Update cfos AddrGrp #AddrGrp has an member with name "none" headers := { "Content-Type": "application/json", } addrgrpbody := { "data": {"name": src_addr_group, "member": [{"name": "none"}]} } addrGroupResp := http.send({ "method": "POST", "url": firewalladdrgrpapiurl, "headers": headers, "body": addrgrpbody }) #End Update cfos AddrGrp #Begin of Firewall Policy update firewallPolicybody := { "data": {"policyid":policyid, "name": policyname, "srcintf": [{"name": srcintf}], "dstintf": [{"name": outgoingport}], "srcaddr": [{"name": src_addr_group}], #"service": [{"name":"ALL"}], "service": myservice, "nat":"enable", "utm-status":utmstatus, "action": "accept", "logtraffic": "all", "ssl-ssh-profile": sslsshprofile, "ips-sensor": ipsprofile, "webfilter-profile": "default", "av-profile": avprofile, "dstaddr": [{"name": "all"}] } } firewallPolicyResp := http.send({ "method": "POST", "url":firewallpolicyapiurl, "headers": headers, "body": firewallPolicybody }) #End of Firewall Policy Update msg :=sprintf( "\\n{%v %v %v} ", [ addrGroupResp.status_code, firewallPolicyResp.status_code, myservice ] ) } EOF kubectl create -f $filename Validate the result kubectl get constrainttemplates -o yaml output will be similar as below
`,description:"",tags:null,title:"Task 13 - Install gatekeeperv3 constraint template",uri:"/06chapter6/14_task13.html"},{content:`Install policy constraint The policy constraint is defined to watch respective API, for example, here we watch NetworkPolicy API. It also functions as parameter input to constraint template.
For example, user passed in policy id=200 for constraint template, which we also pass in cFOS restAPI URL etc.
Note We are using dns name of clusterIP for cFOS API.
If we are not using shared storage for cFOS /data folder, we need to run API call multiple times to make sure that the config is applied for each cFOS POD.. Below command will install policy constraint template
cat \u003c\u003c EOF | kubectl create -f - apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sEgressNetworkPolicyToCfosUtmPolicy metadata: name: cfosnetworkpolicy spec: enforcementAction: deny match: kinds: - apiGroups: ["networking.k8s.io"] kinds: ["NetworkPolicy"] parameters: firewalladdressapiurl : "http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/address" firewallpolicyapiurl : "http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy" firewalladdressgrpapiurl: "http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/addrgrp" policyid : "200" label: "cfosegressfirewallpolicy" outgoingport: "eth0" utmstatus: "enable" ipsprofile: "default" avprofile: "default" sslsshprofile: "deep-inspection" action: "permit" srcintf: "any" extraservice: "PING" EOF Validate the result kubectl get k8segressnetworkpolicytocfosutmpolicy -o yaml output will be similar as below
`,description:"",tags:null,title:"Task 14 - Install policy constraint",uri:"/06chapter6/15_task14.html"},{content:'Create standard NetworkPolicy In here, we create standard K8s egress networkpolicy.\nThis policy will be created on cFOS with the help of gatekeeper.\nNote Once created, policy will not show up when executing the command **kubectl get networkpolicy** created on cFOS.\nBut, one can get the policy info by using the cFOS API with command "kubectl exec -it po/policymanager -- curl -X GET http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy" Below command will deploy networkpolicy\n[[ -z $cfos_label ]] \u0026\u0026 cfos_label="fos" [[ -z $gatekeeper_policy_id ]] \u0026\u0026 gatekeeper_policy_id="200" filename="49_network_firewallpolicy_egress.yml" cat \u003c\u003c EOF \u003e$filename apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: createdbygatekeeper labels: app: cfosegressfirewallpolicy spec: podSelector: matchLabels: app: multitool namespace: default egress: - to: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: TCP port: 443 - protocol: TCP port: 80 EOF #node_list=$(kubectl get nodes -o=jsonpath=\'{range .items[*]}{.metadata.name}{"\\n"}{end}\') node_list=$(kubectl get pod -l app=$cfos_label -o jsonpath=\'{.items[*].status.podIP}\') for node in $node_list; do { while true ; do kubectl apply -f $filename sleep 5 number_of_cfos_pod_applied=$(kubectl exec -it po/policymanager -- curl -X GET "$node/api/v2/cmdb/firewall/policy/$gatekeeper_policy_id" | grep policyid | wc -l) echo number_of_cfos_pod_applied is $number_of_cfos_pod_applied if [ $number_of_cfos_pod_applied -eq 1 ]; then break fi done } done Note Ignore the Error from server (Forbidden) messages. Validate the result kubectl exec -it po/policymanager -- curl -X GET http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy \u0026\u0026 kubectl exec -it po/policymanager -- curl -X GET http://fos-deployment.default.svc.cluster.local/api/v2/cmdb/firewall/policy output will be similar as below\n',description:"",tags:null,title:"Task 15 - Create standard NetworkPolicy",uri:"/06chapter6/16_task15.html"},{content:`Restart Application Deployment Restart application deployment to trigger policymanager update addressgrp in cFOS due to limitation of policymanager, it require pod ip change to trigger update addressgrp in cFOS, we can restart application pod, scale in, scale out etc to force pod IP change. you can use “kubectl logs -f po/policymanager” to check the log of policymanager
Below command will install gatekeeper
kubectl rollout restart deployment multitool01-deployment \u0026\u0026 kubectl rollout status deployment multitool01-deployment echo "sleep 30 seconds for it will take some time to trigger policymanager to update cfos addressgrp" sleep 30 Check gatekeeper installation status kubectl rollout status deployment multitool01-deployment output will be similar as below
`,description:"",tags:null,title:"Task 16 - Restart Application Deployment",uri:"/06chapter6/17_task16.html"},{content:`Perform IPS test Now, the policy created by policymanager will take the action.
When we can check the IPS logs, it shows the traffic matching a different policy ID which is 200.
Below command will send malicous traffic from application pod
kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- dig www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- ping -c 2 www.hackthebox.eu ; done kubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl --max-time 5 -k -H "User-Agent: () { :; }; /bin/ls" https://www.hackthebox.eu ; done Validate the result kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/ips.0 | grep policyid=200 ; done output will be similar as below
`,description:"",tags:null,title:"Task 17 - Perform IPS test",uri:"/06chapter6/18_task17.html"},{content:"Perform Web Filter test Below command will generate traffic to target website\nkubectl get pod | grep multi | grep -v termin | awk '{print }' | while read line; do kubectl exec -t po/$line -- curl -k -I https://www.casino.org ; done Check status kubectl get pod | grep fos | awk '{print $1}' | while read line; do kubectl exec -t po/$line -- tail /data/var/log/log/webf.0 | grep policyid=200 ; done output will be similar as below\n",description:"",tags:null,title:"Task 18 - Perform Web Filter test",uri:"/06chapter6/19_task18.html"},{content:`Destroy GKE Cluster \u0026 Network Below command will Destroy the cluster
gcloud container clusters delete my-first-cluster-1 --zone us-central1-a --quiet \u0026\u0026 gcloud compute firewall-rules delete gkenetwork-allow-custom --quiet \u0026\u0026 gcloud compute networks subnets delete gkenode --region us-central1 --quiet \u0026\u0026 gcloud compute networks delete gkenetwork --quiet output will be similar as below
`,description:"",tags:null,title:"Task 19 - Destroy GKE Cluster \u0026 Network",uri:"/06chapter6/20_task19.html"},{content:`Chapter 2: Create GKE Cluster Tasks
Create \u0026 Validate VPC Network for GKE Cluster Create GKE Cluster Validate GKE Cluster Enable ‘ipforwarding’ on Worker node `,description:"",tags:null,title:"Chapter 2 - Create GKE Cluster",uri:"/02chapter2.html"},{content:`Chapter 3 - Install Multus CNI Tasks
Install Multus CNI Validate Multus CNI installation Create \u0026 Validate net-attach-def for cFOS Create \u0026 Validate net-attach-def for application Validate net-attach-def again `,description:"",tags:null,title:"Chapter 3 - Multus CNI Installation",uri:"/03chapter3.html"},{content:`Chapter 4 - Create Demo application Tasks
Create Demo Application deployment Validate Demo Application deployment `,description:"",tags:null,title:"Chapter 4 - Demo application deployment",uri:"/04chapter4.html"},{content:`Chapter 5 - cFOS Installation Tasks
Create and apply license for cFOS Create \u0026 Validate Role and Service Account for cFOS Create \u0026 Validate cFOS DaemonSet Check Routing Table and IP Address Validate cFOS license Create configmap for cFOS to configure firewall policy Restart \u0026 Validate cFOS DaemonSet Check ping result `,description:"",tags:null,title:"Chapter 5 - cFOS Installation",uri:"/05chapter5.html"},{content:`Chapter 6 - Validate Security features Tasks
Perform initial IPS test
Perform initial Web Filter test
Delete firewall policy using cFOS API
Create a POD to update POD source IP to cFOS
Perform IPS test for 2nd time
Perform Web Filter test for 2nd time
Modify Worker Node default CNI
Delete current application deployment
Create application deployment
Perform Web Filter test for 3rd time
cFOS REST API to delete firewall policy
Install \u0026 Validate gatekeeperv3
Install policy constraint
Create standard NetworkPolicy
Restart Application Deployment
Perform IPS test
Install \u0026 Validate gatekeeperv3
Install \u0026 Validate Policy Constraint
Re-start application deployment
Perform Web Filter test for 4th time on a target website
Delete GKE Cluster
`,description:"",tags:null,title:"Chapter 6 - Validate Security features",uri:"/06chapter6.html"},{content:"",description:"",tags:null,title:"Categories",uri:"/categories.html"},{content:"",description:"",tags:null,title:"Tags",uri:"/tags.html"}]